<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>Toxicwatch Models</title>
    <link rel="stylesheet" href="{{ url_for('static', filename='css/style.css') }}" />
    <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>

</head>
<body>
        <!-- Side navigation -->
        <div class="sidenav">
                <!-- <a href="index.html">Home</a> -->
                <li><a href="/">Home</a></li>
                <li><a href="/about">About</a></li>
                <li><a href="/models">Models</a></li>
                <li><a href="/research">Research</a></li>
                <li><a href="/data">Data</a></li>
                <li><a href="/roadmap">...and beyond</a></li>

                <!-- <a href="roadmap.html">...and beyond</a> -->
              </div>
              <div class="main">
                    <header class="masthead">
                          <div class="container d-flex h-100 align-items-center">
                            <div class="mx-auto text-center">
                                <h4>Models<img id="img2" src="../static/assets/color_logo_transparent.png" class="img-fluid" alt="" style="width: 200px; height: 240px"></h1>
                              <!-- <h1 class="mx-auto my-0 text-uppercase">Toxicwatch</h1> -->
                              <!-- <h2 class="text-white-50 mx-auto mt-2 mb-5">Discover the toxicity of your comments!</h2> -->
                            </div>
                          </div>
                        </header><br><br><br><br><br><br><br><br>
                        <!-- <div class="row"> -->
                                <div >
                                  <h2 id="what_is" class="text-white mb-4">A Model</h2>
                                  <p id="one" class="text-white-50">
                                      Our objective with our A-model was to classify comments based on our dictionary of toxic classes. The NaiveBayes classifier was used to accomplish this task due to its ability to detect the existence or nonexistence of features for a class. But in detecting these features, it doesn't relate them to the existence or nonexistence of any other features of a class. In summary, it is naive because it makes several assumptions about the data-set. However, this classifier was an interesting approach due to the subjective nature of our data.

                                </p>
                                <!-- <section id="left_chart">
                                    <div id='naive_pie' class='col-lg-12' style="width:100%; height: 500px;"></div>
                            
                                    </section> -->
                                    <section id="right_chart">
                                        <div id='naive_pie_complete' class='col-lg-12' style="width:100%; height: 500px;"></div>
                                
                                        </section><br>
                                        <section id="left_chart">
                                            <div id='naive_complete' class='col-lg-12' style="width:100%; height: 500px;"></div>
                                    
                                            </section><br>
                                </div><br>
              
                                <div >
                                  <h2 id="what_is" class="text-white mb-4">J Model</h2>
                                  <p id="one" class="text-white-50"> 
                                      The logistic regression model was used due to its response variable needing to be categorical. Our objective was to create a binary response declaring a comment as toxic or non-toxic. To achieve this, in our J-model we ran all the comments through VADER sentiment analysis to assign scores to assess the degree of positive/negative/neutral word selection within the data-set. From this, we were able to feed our logistic regression model with the independent variables as the toxic comments' compound score, negative score, neutral score, and positive score. The dependent variable was set as the Toxic column for our data-set. The target output of this model was to determine if the comment is toxic or nontoxic based on a combination of the Vader scores.
                                      <br><br>
                                      There's a 6.25% difference in toxicity percentages reported for Actual vs Predicted toxicity. With the former reporting more toxic comments.                              
                                  </p>
                                  <section id="left_chart">
                                        <div id='pie_actual' class='col-lg-12' style="width:100%; height: 450px;"></div>
                                
                                        </section>
                                        <section id="right_chart">
                                          <div id='pie_predict' class='col-lg-12' style="width:100%; height: 450px;"></div>
                                          </section>
                                          
                                        </div>
                                  <h2 id="what_is" class="text-white mb-4">R Model</h2>
                                  <p id="one" class="text-white-50">  
                                      For Linear regression we wanted to see the relation between a explanatory variable and a dependent variable.  The purpose is to be able to fit a linear equation to observe and predict data and see how one variable affects the other.  The linear regression line Y = a + bX where X is the explanatory variable and Y is the dependant variable.  In this case the “Toxic” column is Y for the dependent variable as it’s dependent on the Hash Values which represents X for the hash values.  We wanted to use this to determine if a relationship exists between certain words based on how they’re hashed and whether or not the comment as a whole can be deemed toxic.                              
                                  </p>
                      
                      
                                <!-- </div> -->
                              
                </div>
                       <!-- Footer -->
    <section id="footer">

   
        <footer class="bg-black small text-center text-white-50">
          <div class="container">
            Copyright &copy; Coding Bootcamp 2018
          </div>
        </footer>
      </section>
         
                <script type='text/javascript'src="../static/js/pie.js"></script>
                <script type='text/javascript'src="../static/js/pie_predict.js"></script>
                <script type='text/javascript'src="../static/js/naive_pie.js"></script>
                <script type='text/javascript'src="../static/js/naive_pie_complete.js"></script>
                <script type='text/javascript'src="../static/js/naive_complete.js"></script>




</body>
</html>